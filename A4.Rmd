---
title: "Assignment 4 - (Tree-based Methods) Data Mining and Statistical Learning"
author: "Ziwen Zhang"
date: "2023-07-10"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    fig_height: 6.5
    fig_width: 9.5
---

## Dataset
Use the "Heart" dataset saved in the module that contains a binary outcome AHD for 303 patients who presented with chest pain. An outcome value of Yes indicates the presence of heart disease based on an angiographic test, while No means no heart disease. There are 13 predictors including Age, Sex, Chol (a cholesterol measurement), and other heart and lung function measurements.

```{r, warning=FALSE}
# load the data set
heart <- read.csv("heart.csv")[,-1]

# view data type
str(heart)

# change datatype into factor
heart$Thal <- as.factor(heart$Thal)
heart$AHD <- as.factor(heart$AHD)
heart$ChestPain <- as.factor(heart$ChestPain)

# load libraries
library(tidyverse)
library(tree)
library(randomForest)
library(gbm)
```


## Question
With the data set, fit the following tree-based predictive model - 

### Question (1) CART Model
A single classification and regression tree that has been pruned back adequately. Explain in details the steps taken to prune the tree.
```{r, warning=FALSE}
# impute the missing data
heart_imputed <- rfImpute(AHD ~ .-AHD, data = heart)

set.seed(24543)
# divide data into testing and training 50%
test_index <- sample(nrow(heart_imputed), ceiling(0.5*nrow(heart_imputed)))
test_heart <- heart_imputed[test_index, -1] # eliminate the target
train_heart <- heart_imputed[-test_index, ]

test_label <- as.numeric(heart_imputed[test_index, 1]) # set as target
test_full <- heart_imputed[test_index, ]
```

#### fit a classification tree
```{r}
set.seed(765)
# build a single classification and regression tree
tree_model <- tree(AHD ~.-AHD, data=train_heart, method="gini")
tree_model.cv <- cv.tree(tree_model, FUN=prune.misclass, K=10)

# plot the error as a function of the tree size
plot(dev ~ size, data=as.data.frame(tree_model.cv[1:3]), type="b")
points(x=tree_model.cv$size[tree_model.cv$dev==min(tree_model.cv$dev)],
y=rep(min(tree_model.cv$dev),sum(tree_model.cv$dev==min(tree_model.cv$dev))),col="red",pch=19)
```

From the plot, we can see that the red point demonstrates the tree size with the smallest error.

#### finalize a classification tree
```{r}
final.tree_model <- prune.tree(tree_model, best=tree_model.cv$size[tree_model.cv$dev==min(tree_model.cv$dev)])
plot(final.tree_model); text(final.tree_model,pretty=3,digits=3)
```

**Explain in details the steps taken to prune the tree: **
Firstly, we need to split out training dataset and testing dataset from the original dataset.
Secondly, we need to build a CART model on the training dataset, this tree has no limitations on its size or complexity.
Thirdly, we need to calculate the impurity measures (such as Gini Index) to evaluate the homogeneity of the samples within each node. 
Fourthly, we can apply cross-validation to improve the model performance.
Fifthly, we can calculate the error on the testing dataset, and plot the result so that the optimal size of trees will be distinguished.
Finally, we can prune the tree according to the previous plot, and also apply cross-validation to improve the model performance.

### Question (2) Random Forest & Bagged Tree
A random forest of with m = sqrt(p) and a bagged tree with m = p.

#### fit a random forest of with m = sqrt(p)
```{r}
# fit a random forest model on the training data
rf_model <- randomForest(AHD~.-AHD, data=train_heart, mtry=sqrt(13), ntree=500, importance=TRUE)

# calculate and plot importance measures 
importance(rf_model)
varImpPlot(rf_model)
```

#### fit a bagged tree with m = p
```{r}
# fit a bagged tree model on the training data
bagged_tree <- randomForest(AHD~.-AHD, data=train_heart, mtry=13, ntree=500, importance=TRUE)

importance(bagged_tree)
varImpPlot(bagged_tree)
```


### Question (3) Gradient Boosted Model
A gradient boosted model with depth one and two.
```{r}
library(gbm)

# change the target binary variable in to zero and one
heart$AHD <- ifelse(heart$AHD == "Yes", 1, 0)
```

#### fit a gradient boosted model with depth one
```{r}
# fit a gradient boosting model with depth one
gbm_model_1 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "bernoulli",
  interaction.depth = 1,
  n.trees = 500,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(gbm_model_1)
```

```{r}
## Best number of trees via OOB
gbm.perf(gbm_model_1, method="OOB") # ntree = 308
```

```{r}
# fit a final gradient boosting model with ntree=308
gbm_final_1 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "bernoulli",
  interaction.depth = 1,
  n.trees = 308,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(gbm_final_1)
```

#### fit a gradient boosted model with depth two
```{r}
# fit a gradient boosting model with depth two
gbm_model_2 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "bernoulli",
  interaction.depth = 2,
  n.trees = 500,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(gbm_model_2)
```

```{r}
## Best number of trees via OOB
gbm.perf(gbm_model_2, method="OOB") # ntree = 247
```

```{r}
# fit a final gradient boosting model with ntree=247
gbm_final_2 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "bernoulli",
  interaction.depth = 1,
  n.trees = 247,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(gbm_final_2)
```

### Question (4) Adaboost Model
An Adaboost model with depth one and two.

#### fit an Adaboost model with depth one
```{r}
# fit an Adaboost model with depth one
ada_model_1 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "adaboost",
  interaction.depth = 1,
  n.trees = 500,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(ada_model_1)
```

```{r}
## Best number of trees via OOB
gbm.perf(ada_model_1, method="OOB") # ntree = 264
```

```{r}
# fit a final adaboost model with ntree=264
ada_final_1 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "adaboost",
  interaction.depth = 1,
  n.trees = 264,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(ada_final_1)
```

#### fit an Adaboost model with depth two
```{r}
# fit an Adaboost model with depth two
ada_model_2 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "adaboost",
  interaction.depth = 2,
  n.trees = 500,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(ada_model_2)
```

```{r}
## Best number of trees via OOB
gbm.perf(ada_model_2, method="OOB") # ntree = 233
```

```{r}
# fit a final adaboost model with ntree=233
ada_final_2 <- gbm(
  formula = AHD ~ .,
  data = heart,
  distribution = "adaboost",
  interaction.depth = 1,
  n.trees = 233,
  shrinkage = 0.01, 
  train.fraction = 0.5
)

# summarize the model
summary(ada_final_2)
```


### Question (5) Misclassification error
Compare the Test misclassification error of all these methods. For the methods (2-4) plot the Test misclassification error as a function of the number of trees used for these methods. (Note: there might be tuning parameters that you need to choose for some of these methods and you need to perform cross-validation within the training data in order to choose those parameters. In other words, you should perform CV to choose the value of shrinkage parameter in gbm and adaboost.).

#### Method 1: single tree model
```{r}
# generate predicted value on testing dataset
pred_tree_model <- predict(final.tree_model, newdata=test_heart, type="class")
tmp <- table(pred_tree_model, test_label)
tmp

# calculate the mis-classification error
mse_tree_model <- round(1-sum(diag(tmp)/sum(tmp)), 4)
paste("the misclassification error of classification tree is: ", mse_tree_model)
```

#### Method 2: Random Forest and Bagged Tree
```{r}
# generate predicted value on testing data set
pred_rf_model <- predict(rf_model, newdata = test_heart, type = "response")

# calculate the misclassification error on this test data
pred_label_rf <- ifelse(pred_rf_model == "Yes", 2, 1)
mse_rf_model <- mean(pred_label_rf != test_label) %>% round(4)
paste("the misclassification error of random forest is: ", mse_rf_model)


# generate predicted value on testing data set
pred_bagged_model <- predict(bagged_tree, newdata = test_heart, type = "response")

# calculate the misclassification error on this test data
pred_label_bagged <- ifelse(pred_bagged_model == "Yes", 2, 1)
mse_bagged_model <- mean(pred_label_bagged != test_label) %>% round(4)
paste("the misclassification error of bagged tree is: ", mse_bagged_model)

# mse of cross validation
rf_cv <- rfcv(trainx = train_heart[, -1], trainy = train_heart[, 1], cv.fold = 10)
rf_cv_error <- mean(rf_cv$error.cv) %>% round(4)
paste("the misclassification error of cross-validation is: ", rf_cv_error)
```

##### plot the Test misclassification error as a function of the number of trees

```{r}
ntree <- seq(500, 20000, by=1000)
mse_rf <- c()

for (i in ntree){
  rf_model = randomForest(AHD ~ ., data = train_heart, mtry=sqrt(13), ntree=i, importance=TRUE)
  pred_rf = predict(rf_model, newdata = test_heart, type = "response")
  pred_label = ifelse(pred_rf == "Yes", 2, 1)
  mse_rf_seq <- mean(pred_label != test_label) %>% round(4)
  mse_rf_all = append(mse_rf, mse_rf_seq)
}

plot(ntree, mse_rf, type = "l", xlim = c(0, 20000), ylim = c(0, 0.5))
```

#### Method 3: gradient boosted model
```{r}
# gbm_cv <- gbm(
#   formula = AHD ~ .,
#   data = heart,
#   distribution = "bernoulli",
#   interaction.depth = 1,
#   n.trees = 5000,
#   shrinkage = 0.01, 
#   train.fraction = 0.5, 
#   cv.folds = 10
# )
# 
# # plot the cv error versus number of trees
# gbm.perf(gbm_cv, method="cv")

# obtain predicted values on testing data
pred_gbm_1 <- predict.gbm(gbm_final_1, newdata = test_heart, n.trees = 308, type = "response")

# calculate the misclassification error on this test data
pred_label_gbm_1 <- ifelse(pred_gbm_1 == "Yes", 2, 1)
mse_gbm_1 <- mean(pred_label_gbm_1 != test_label) %>% round(4)
paste("the misclassification error of gradient boosting model with depth 1 is: ", mse_gbm_1)


# obtain predicted values on testing data
pred_gbm_2 <- predict.gbm(gbm_final_2, newdata = test_heart, n.trees = 247, type = "response")

# calculate the misclassification error on this test data
pred_label_gbm_2 <- ifelse(pred_gbm_2 == "Yes", 2, 1)
mse_gbm_2 <- mean(pred_label_gbm_2 != test_label) %>% round(4)
paste("the misclassification error of gradient boosting model with depth 2 is: ", mse_gbm_2)
```

#### Method 4: adaboosting model
```{r}
# ada_cv <- gbm(
#   formula = AHD ~ .,
#   data = heart,
#   distribution = "adaboost",
#   interaction.depth = 1,
#   n.trees = 5000,
#   shrinkage = 0.01, 
#   train.fraction = 0.5, 
#   cv.folds = 10
# )
# 
# # plot the cv error versus number of trees
# gbm.perf(ada_cv, method="cv")

# obtain predicted values on testing data
pred_ada_1 <- predict.gbm(ada_final_1, newdata = test_heart, n.trees = 264, type = "response")

# calculate the misclassification error on this test data
pred_label_ada_1 <- ifelse(pred_ada_1 == "Yes", 2, 1)
mse_ada_1 <- mean(pred_label_ada_1 != test_label) %>% round(4)
paste("the misclassification error of adaboost model with depth 1 is: ", mse_ada_1)


# obtain predicted values on testing data
pred_ada_2 <- predict.gbm(ada_final_2, newdata = test_heart, n.trees = 233, type = "response")

# calculate the misclassification error on this test data
pred_label_ada_2 <- ifelse(pred_ada_2 == "Yes", 2, 1)
mse_ada_2 <- mean(pred_label_ada_2 != test_label) %>% round(4)
paste("the misclassification error of adaboost model with depth 2 is: ", mse_ada_2)
```


hint: As noted in lab running cross validation built in the gbm() function tries to perform parallel computation for each fold using the n.cores cores of your computer's processor. Depending on the operating system, firewall and system architecture this may or may not work in your computer.